# EIP4


Assignment 2:
### 1. copy and paste your Logs for 20 epochs

Train on 60000 samples, validate on 10000 samples
Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
60000/60000 [==============================] - 43s 716us/step - loss: 0.1242 - acc: 0.9490 - val_loss: 0.0374 - val_acc: 0.9885

Epoch 00001: val_loss improved from inf to 0.03737, saving model to /content/gdrive/My Drive/model-001-0.948967-0.988500.h5
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0024691358.
60000/60000 [==============================] - 14s 241us/step - loss: 0.1104 - acc: 0.9529 - val_loss: 0.0341 - val_acc: 0.9902

Epoch 00002: val_loss improved from 0.03737 to 0.03410, saving model to /content/gdrive/My Drive/model-002-0.952883-0.990200.h5
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0017266685.
60000/60000 [==============================] - 14s 238us/step - loss: 0.1034 - acc: 0.9555 - val_loss: 0.0216 - val_acc: 0.9927

Epoch 00003: val_loss improved from 0.03410 to 0.02157, saving model to /content/gdrive/My Drive/model-003-0.955500-0.992700.h5
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0010496465.
60000/60000 [==============================] - 14s 236us/step - loss: 0.0972 - acc: 0.9558 - val_loss: 0.0219 - val_acc: 0.9939

Epoch 00004: val_loss did not improve from 0.02157
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0005643261.
60000/60000 [==============================] - 14s 238us/step - loss: 0.0928 - acc: 0.9567 - val_loss: 0.0196 - val_acc: 0.9945

Epoch 00005: val_loss improved from 0.02157 to 0.01955, saving model to /content/gdrive/My Drive/model-005-0.956650-0.994500.h5
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0002719644.
60000/60000 [==============================] - 15s 242us/step - loss: 0.0922 - acc: 0.9577 - val_loss: 0.0202 - val_acc: 0.9940

Epoch 00006: val_loss did not improve from 0.01955
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0001187618.
60000/60000 [==============================] - 14s 234us/step - loss: 0.0897 - acc: 0.9585 - val_loss: 0.0193 - val_acc: 0.9943

Epoch 00007: val_loss improved from 0.01955 to 0.01925, saving model to /content/gdrive/My Drive/model-007-0.958483-0.994300.h5
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 4.74099e-05.
60000/60000 [==============================] - 14s 237us/step - loss: 0.0910 - acc: 0.9578 - val_loss: 0.0186 - val_acc: 0.9943

Epoch 00008: val_loss improved from 0.01925 to 0.01858, saving model to /content/gdrive/My Drive/model-008-0.957817-0.994300.h5
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 1.74301e-05.
60000/60000 [==============================] - 14s 234us/step - loss: 0.0894 - acc: 0.9583 - val_loss: 0.0187 - val_acc: 0.9942

Epoch 00009: val_loss did not improve from 0.01858
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 5.9387e-06.
60000/60000 [==============================] - 14s 233us/step - loss: 0.0892 - acc: 0.9575 - val_loss: 0.0184 - val_acc: 0.9946

Epoch 00010: val_loss improved from 0.01858 to 0.01838, saving model to /content/gdrive/My Drive/model-010-0.957450-0.994600.h5
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 1.8853e-06.
60000/60000 [==============================] - 14s 232us/step - loss: 0.0897 - acc: 0.9583 - val_loss: 0.0186 - val_acc: 0.9944

Epoch 00011: val_loss did not improve from 0.01838
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 5.603e-07.
60000/60000 [==============================] - 14s 235us/step - loss: 0.0889 - acc: 0.9582 - val_loss: 0.0185 - val_acc: 0.9945

Epoch 00012: val_loss did not improve from 0.01838
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 1.565e-07.
60000/60000 [==============================] - 14s 233us/step - loss: 0.0895 - acc: 0.9580 - val_loss: 0.0186 - val_acc: 0.9944

Epoch 00013: val_loss did not improve from 0.01838
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 4.12e-08.
60000/60000 [==============================] - 14s 232us/step - loss: 0.0893 - acc: 0.9583 - val_loss: 0.0185 - val_acc: 0.9946

Epoch 00014: val_loss did not improve from 0.01838
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 1.03e-08.
60000/60000 [==============================] - 14s 236us/step - loss: 0.0857 - acc: 0.9599 - val_loss: 0.0185 - val_acc: 0.9946

Epoch 00015: val_loss did not improve from 0.01838
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 2.4e-09.
60000/60000 [==============================] - 14s 235us/step - loss: 0.0883 - acc: 0.9580 - val_loss: 0.0186 - val_acc: 0.9945

Epoch 00016: val_loss did not improve from 0.01838
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 5e-10.
60000/60000 [==============================] - 14s 233us/step - loss: 0.0915 - acc: 0.9571 - val_loss: 0.0186 - val_acc: 0.9944

Epoch 00017: val_loss did not improve from 0.01838
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 1e-10.
60000/60000 [==============================] - 14s 230us/step - loss: 0.0883 - acc: 0.9576 - val_loss: 0.0185 - val_acc: 0.9945

Epoch 00018: val_loss did not improve from 0.01838
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0.
60000/60000 [==============================] - 14s 234us/step - loss: 0.0891 - acc: 0.9587 - val_loss: 0.0184 - val_acc: 0.9946

Epoch 00019: val_loss did not improve from 0.01838
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.0.
60000/60000 [==============================] - 14s 234us/step - loss: 0.0893 - acc: 0.9579 - val_loss: 0.0185 - val_acc: 0.9946

Epoch 00020: val_loss did not improve from 0.01838
<keras.callbacks.History at 0x7f8ef55c84e0>


### 2. copy and paste the result of your model.evaluate (on test data)
Total params: 14,692
Trainable params: 14,464
Non-trainable params: 228
[0.018506120436755008, 0.9946]

3. strategy you have taken to achieve the said results

1. I first tried to reduce initial layers of features identifiers size and tried to optimise it and see if there is any difference it would make
2. I tried changin lr with various optimiser parameters 
def scheduler(epoch, lr):
  return round(lr * 1/(1 +  0.215 * epoch), 10) #0.319
  It did made some difference in getting consistent result
3. I am yet to get an intuition on effect of receptive fields on this dataset and i am not seeing significant results in reducing the parameters in later fields.
4. I did not try Image Augmentation as it was not perview of the assignment, i think it would have made the results much consistent with point 3.


